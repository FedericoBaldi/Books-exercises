{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING DATA\n",
    "# To do anything you need data. let'see how and where we can get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "\n",
    "# a small python script if run in command line reads lines of text and write out the ones that match a regular expression:\n",
    "# sys.argv is the list of command-line arguments\n",
    "# sys.argv[0] is the name of the program itself\n",
    "# sys.argv[1] will be the regex specified at the command line\n",
    "regex = sys.argv[1]\n",
    "# for every line passed into the script\n",
    "for line in sys.stdin:\n",
    "    # if it matches the regex, write it to stdout\n",
    "    if re.search(regex, line):\n",
    "        sys.stdout.write(line)\n",
    "\n",
    "# another python script that counts the lines it receives and then writes out the count:\n",
    "# line_count.py\n",
    "import sys\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "# print goes to sys.stdout\n",
    "print(count)\n",
    "\n",
    "# you can use both to count how many lines of a file contain numbers:\n",
    "# type SomeFile.txt | egrep.py \"[0-9]\" | line_count.py\n",
    "\n",
    "# or again a script that counts the words in its input and writes out the most common ones:\n",
    "# most_common_words.py\n",
    "import sys\n",
    "from collections import Counter\n",
    "# pass in number of words as first argument\n",
    "try:\n",
    "    num_words = int(sys.argv[1])\n",
    "except:\n",
    "    print(\"usage: most_common_words.py num_words\")\n",
    "    sys.exit(1)   # nonzero exit code indicates error\n",
    "counter = Counter(word.lower()                      # lowercase words\n",
    "                  for line in sys.stdin\n",
    "                  for word in line.strip().split()  # split on spaces\n",
    "                  if word)                          # skip empty 'words'\n",
    "for word, count in counter.most_common(num_words):\n",
    "    sys.stdout.write(str(count))\n",
    "    sys.stdout.write(\"\\t\")\n",
    "    sys.stdout.write(word)\n",
    "    sys.stdout.write(\"\\n\")\n",
    "\"\"\" type the_bible.txt | most_common_words.py 10\n",
    "36397\tthe\n",
    "30031\tand\n",
    "20163\tof\n",
    "7154\tto\n",
    "6484\tin\n",
    "5856\tthat\n",
    "5421\the\n",
    "5226\this\n",
    "5060\tunto\n",
    "4297\tshall \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read files we can use \"open\"\n",
    "# 'r' means read-only, it's assumed if you leave it out\n",
    "file_for_reading = open('reading_file.txt', 'r')\n",
    "file_for_reading2 = open('reading_file.txt')\n",
    "\n",
    "# 'w' is write -- will destroy the file if it already exists!\n",
    "file_for_writing = open('writing_file.txt', 'w')\n",
    "\n",
    "# 'a' is append -- for adding to the end of the file\n",
    "file_for_appending = open('appending_file.txt', 'a')\n",
    "\n",
    "# don't forget to close your files when you're done\n",
    "file_for_writing.close()\n",
    "\n",
    "#since it is easy to forget to close files it's better to use \"with\"\n",
    "starts_with_hash = 0\n",
    "with open('input.txt') as f:\n",
    "    for line in f:                  # look at each line in the file\n",
    "        if re.match(\"^#\",line):     # use a regex to see if it starts with '#'\n",
    "            starts_with_hash += 1   # if it does, add 1 to the count\n",
    "\n",
    "# as an example we can extract the domains from a file with a list of email addresses\n",
    "def get_domain(email_address: str) -> str:\n",
    "    \"\"\"Split on '@' and return the last piece\"\"\"\n",
    "    return email_address.lower().split(\"@\")[-1]\n",
    "\n",
    "# a couple of tests\n",
    "assert get_domain('joelgrus@gmail.com') == 'gmail.com'\n",
    "assert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com'\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "with open('email_addresses.txt', 'r') as f:\n",
    "    domain_counts = Counter(get_domain(line.strip())\n",
    "                            for line in f\n",
    "                            if \"@\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle delimited files like csv, don't do it yourself, use a csv reader\n",
    "# a file tab separated without headers\n",
    "\"\"\"\n",
    "6/20/2014   AAPL    90.91\n",
    "6/20/2014   MSFT    41.68\n",
    "6/20/2014   FB  64.5\n",
    "6/19/2014   AAPL    91.86\n",
    "6/19/2014   MSFT    41.51\n",
    "6/19/2014   FB  64.34\n",
    "\"\"\"\n",
    "import csv\n",
    "with open('tab_delimited_stock_prices.txt') as f:\n",
    "    tab_reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in tab_reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "# a file colon separated with headers\n",
    "\"\"\"\n",
    "date:symbol:closing_price\n",
    "6/20/2014:AAPL:90.91\n",
    "6/20/2014:MSFT:41.68\n",
    "6/20/2014:FB:64.5\n",
    "\"\"\"\n",
    "with open('colon_delimited_stock_prices.txt') as f:\n",
    "    colon_reader = csv.DictReader(f, delimiter=':')\n",
    "    for dict_row in colon_reader:\n",
    "        date = dict_row[\"date\"]\n",
    "        symbol = dict_row[\"symbol\"]\n",
    "        closing_price = float(dict_row[\"closing_price\"])\n",
    "# if your file does not have headers you can use \"DictReder\" by passing the keys\n",
    "todays_prices = {'AAPL': 90.91, 'MSFT': 41.68, 'FB': 64.5 }\n",
    "with open('comma_delimited_stock_prices.txt', 'w') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',')\n",
    "    for stock, price in todays_prices.items():\n",
    "        csv_writer.writerow([stock, price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to get data is by scraping the web. BeautifulSoup is a usefull and easy library we can use\n",
    "# in HTML we have structured data. In an ideal world we could find all the tags starting with <p and use the ids(that sometimes are missing).\n",
    "\"\"\"\n",
    "<html>\n",
    "  <head>\n",
    "    <title>A web page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p id=\"author\">Joel Grus</p>\n",
    "    <p id=\"subject\">Data Science</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Example HTML file on GitHub. In order to fit\n",
    "# the URL in the book I had to split it across two lines.\n",
    "# Recall that whitespace-separated strings get concatenated.\n",
    "url = (\"https://raw.githubusercontent.com/\"\n",
    "       \"joelgrus/data/master/getting-data.html\")\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "first_paragraph = soup.find('p')        # or just soup.p\n",
    "first_paragraph_text = soup.p.text      # get the text of a tag\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "first_paragraph_id = soup.p['id']       # raises KeyError if no 'id'\n",
    "first_paragraph_id2 = soup.p.get('id')  # returns None if no 'id'\n",
    "# you could need to find tags with a specific class\n",
    "important_paragraphs = soup('p', {'class' : 'important'})\n",
    "important_paragraphs2 = soup('p', 'important')\n",
    "important_paragraphs3 = [p for p in soup('p')\n",
    "                         if 'important' in p.get('class', [])]\n",
    "# or if you want to find every <span> contained inside a <div>\n",
    "# Warning: will return the same <span> multiple times\n",
    "# if it sits inside multiple <div>s.\n",
    "# Be more clever if that's the case.\n",
    "spans_inside_divs = [span\n",
    "                     for div in soup('div')     # for each <div> on the page\n",
    "                     for span in div('span')]   # find each <span> inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JavaScript', 'Python', 'Python', 'Python', 'Python']\n"
     ]
    }
   ],
   "source": [
    "# many websites provide APIs, which allow you to explicitly request data in a structured format.\n",
    "# usually they are JSON formatted\n",
    "import json\n",
    "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                  \"author\" : \"Joel Grus\",\n",
    "                  \"publicationYear\" : 2019,\n",
    "                  \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "# parse the JSON to create a Python dict\n",
    "deserialized = json.loads(serialized)\n",
    "assert deserialized[\"publicationYear\"] == 2019\n",
    "assert \"data science\" in deserialized[\"topics\"]\n",
    "\n",
    "# let's start with unauthenticated GitHub APIs\n",
    "from collections import Counter\n",
    "from dateutil.parser import parse\n",
    "github_user = \"joelgrus\"\n",
    "endpoint = f\"https://api.github.com/users/{github_user}/repos\"\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "# get created dates of all repos\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "\n",
    "# get the languages of the last 5 repositories\n",
    "last_5_repositories = sorted(repos,\n",
    "                             key=lambda r: r[\"pushed_at\"],\n",
    "                             reverse=True)[:5]\n",
    "last_5_languages = [repo[\"language\"]\n",
    "                    for repo in last_5_repositories]\n",
    "print(last_5_languages)\n",
    "# APIs are a very powerfull and usefull tool. \n",
    "# They could be well written and give you a lot of flexibility but unfortunatly sometimes are not updated or not reliable and can give you huge headhaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For Further Exploration\n",
    "pandas is the primary library that data science types use for working with—and, in particular, importing—data.\n",
    "\n",
    "Scrapy is a full-featured library for building complicated web scrapers that do things like follow unknown links.\n",
    "\n",
    "Kaggle hosts a large collection of datasets.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5207bb88b4cad36c522ddd0321c41d9d1435820ac336d39d579f6bdc27d65fa1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
