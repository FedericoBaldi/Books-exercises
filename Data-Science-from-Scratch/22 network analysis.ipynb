{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ANALYSIS\n",
    "# many problem can be associated or modeled like a network. it consists of nodes and edges\n",
    "\n",
    "import data_lib\n",
    "from data_lib import Vector, Matrix\n",
    "from typing import List, NamedTuple, Dict, Tuple\n",
    "from collections import deque, Counter\n",
    "import math\n",
    "import random\n",
    "\n",
    "class User(NamedTuple):\n",
    "    id: int\n",
    "    name: str\n",
    "\n",
    "users = [User(0, \"Hero\"), User(1, \"Dunn\"), User(2, \"Sue\"), User(3, \"Chi\"),\n",
    "         User(4, \"Thor\"), User(5, \"Clive\"), User(6, \"Hicks\"),\n",
    "         User(7, \"Devin\"), User(8, \"Kate\"), User(9, \"Klein\")]\n",
    "\n",
    "friend_pairs = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
    "                (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
    "\n",
    "# type alias for keeping track of Friendships. it's better to use a dictionary to keep track of edges\n",
    "Friendships = Dict[int, List[int]]\n",
    "friendships: Friendships = {user.id: [] for user in users}\n",
    "\n",
    "for i, j in friend_pairs:\n",
    "    friendships[i].append(j)\n",
    "    friendships[j].append(i)\n",
    "\n",
    "assert friendships[4] == [3, 5]\n",
    "assert friendships[8] == [6, 7, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3857798764405388, 0.5147886508200107, 0.5147886508200107, 0.47331358974108806, 0.23360899138459798, 0.15015252204496968, 0.08355492533967541, 0.08355492533967541, 0.07284672570068662, 0.027294420660842957]\n"
     ]
    }
   ],
   "source": [
    "Path = List[int]\n",
    "def shortest_paths_from(from_user_id: int,\n",
    "                        friendships: Friendships) -> Dict[int, List[Path]]:\n",
    "    # A dictionary from user_id to *all* shortest paths to that user.\n",
    "    shortest_paths_to: Dict[int, List[Path]] = {from_user_id: [[]]}\n",
    "    # A queue of (previous user, next user) that we need to check.\n",
    "    # Starts out with all pairs (from_user, friend_of_from_user).\n",
    "    frontier = deque((from_user_id, friend_id)\n",
    "                     for friend_id in friendships[from_user_id])\n",
    "    # Keep going until we empty the queue.\n",
    "    while frontier:\n",
    "        # Remove the pair that's next in the queue.\n",
    "        prev_user_id, user_id = frontier.popleft()\n",
    "        # Because of the way we're adding to the queue,\n",
    "        # necessarily we already know some shortest paths to prev_user.\n",
    "        paths_to_prev_user = shortest_paths_to[prev_user_id]\n",
    "        new_paths_to_user = [path + [user_id] for path in paths_to_prev_user]\n",
    "        # It's possible we already know a shortest path to user_id.\n",
    "        old_paths_to_user = shortest_paths_to.get(user_id, [])\n",
    "\n",
    "        # What's the shortest path to here that we've seen so far?\n",
    "        if old_paths_to_user:\n",
    "            min_path_length = len(old_paths_to_user[0])\n",
    "        else:\n",
    "            min_path_length = float('inf')\n",
    "\n",
    "        # Only keep paths that aren't too long and are actually new.\n",
    "        new_paths_to_user = [path\n",
    "                             for path in new_paths_to_user\n",
    "                             if len(path) <= min_path_length\n",
    "                             and path not in old_paths_to_user]\n",
    "\n",
    "        shortest_paths_to[user_id] = old_paths_to_user + new_paths_to_user\n",
    "        # Add never-seen neighbors to the frontier.\n",
    "        frontier.extend((user_id, friend_id)\n",
    "                        for friend_id in friendships[user_id]\n",
    "                        if friend_id not in shortest_paths_to)\n",
    "\n",
    "    return shortest_paths_to\n",
    "\n",
    "# For each from_user, for each to_user, a list of shortest paths.\n",
    "shortest_paths = {user.id: shortest_paths_from(user.id, friendships)\n",
    "                  for user in users}\n",
    "\n",
    "# we now have all the shortest paths. For each of those paths, we just add 1/n to the centrality of each node on that path\n",
    "betweenness_centrality = {user.id: 0.0 for user in users}\n",
    "for source in users:\n",
    "    for target_id, paths in shortest_paths[source.id].items():\n",
    "        if source.id < target_id:      # don't double count\n",
    "            num_paths = len(paths)     # how many shortest paths?\n",
    "            contrib = 1 / num_paths    # contribution to centrality\n",
    "            for path in paths:\n",
    "                for between_id in path:\n",
    "                    if between_id not in [source.id, target_id]:\n",
    "                        betweenness_centrality[between_id] += contrib\n",
    "\n",
    "def farness(user_id: int) -> float:\n",
    "    \"\"\"the sum of the lengths of the shortest paths to each other user\"\"\"\n",
    "    return sum(len(paths[0])\n",
    "               for paths in shortest_paths[user_id].values())\n",
    "\n",
    "# use farness to calculate the closeness centrality.\n",
    "closeness_centrality = {user.id: 1 / farness(user.id) for user in users}\n",
    "\n",
    "# for big networks these last 2 measures are heavy to compute. A more used measure is eigenvector centrality which uses matrixes\n",
    "def matrix_times_matrix(m1: Matrix, m2: Matrix) -> data_lib.Matrix:\n",
    "    nr1, nc1 = data_lib.shape(m1)\n",
    "    nr2, nc2 = data_lib.shape(m2)\n",
    "    assert nc1 == nr2, \"must have (# of columns in m1) == (# of rows in m2)\"\n",
    "\n",
    "    def entry_fn(i: int, j: int) -> float:\n",
    "        \"\"\"dot product of i-th row of m1 with j-th column of m2\"\"\"\n",
    "        return sum(m1[i][k] * m2[k][j] for k in range(nc1))\n",
    "\n",
    "    return data_lib.make_matrix(nr1, nc2, entry_fn)\n",
    "\n",
    "def matrix_times_vector(m: Matrix, v: Vector) -> Vector:\n",
    "    nr, nc = data_lib.shape(m)\n",
    "    n = len(v)\n",
    "    assert nc == n, \"must have (# of cols in m) == (# of elements in v)\"\n",
    "\n",
    "    return [data_lib.dot(row, v) for row in m]  # output has length nr\n",
    "\n",
    "# remember that not all matrixes have eigenvector/s\n",
    "def find_eigenvector(m: Matrix,\n",
    "                     tolerance: float = 0.00001) -> Tuple[Vector, float]:\n",
    "    guess = [random.random() for _ in m]\n",
    "    while True:\n",
    "        result = matrix_times_vector(m, guess)    # transform guess\n",
    "        norm = data_lib.magnitude(result)                  # compute norm\n",
    "        next_guess = [x / norm for x in result]   # rescale\n",
    "        if data_lib.distance(guess, next_guess) < tolerance:\n",
    "            # convergence so return (eigenvector, eigenvalue)\n",
    "            return next_guess, norm\n",
    "        guess = next_guess\n",
    "\n",
    "# represent connections in an adjacency matrix. every (i,j) is either 1 if user i and user j are friends, or 0 if they are not\n",
    "def entry_fn(i: int, j: int):\n",
    "    return 1 if (i, j) in friend_pairs or (j, i) in friend_pairs else 0\n",
    "\n",
    "n = len(users)\n",
    "adjacency_matrix = data_lib.make_matrix(n, n, entry_fn)\n",
    "\n",
    "# eigenvector centralities are numbers, one per user, such that each user’s value is a constant multiple of the sum of his neighbors’ values\n",
    "eigenvector_centralities, _ = find_eigenvector(adjacency_matrix)\n",
    "print(eigenvector_centralities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directed graph, instead of having friendships we have endorsments\n",
    "endorsements = [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2),\n",
    "                (2, 1), (1, 3), (2, 3), (3, 4), (5, 4),\n",
    "                (5, 6), (7, 5), (6, 8), (8, 7), (8, 9)]\n",
    "endorsement_counts = Counter(target for source, target in endorsements)\n",
    "\n",
    "def page_rank(users: List[User],\n",
    "              endorsements: List[Tuple[int, int]],\n",
    "              damping: float = 0.85,\n",
    "              num_iters: int = 100) -> Dict[int, float]:\n",
    "    # Compute how many people each person endorses\n",
    "    outgoing_counts = Counter(target for source, target in endorsements)\n",
    "    # Initially distribute PageRank evenly\n",
    "    num_users = len(users)\n",
    "    pr = {user.id : 1 / num_users for user in users}\n",
    "    # Small fraction of PageRank that each node gets each iteration\n",
    "    base_pr = (1 - damping) / num_users\n",
    "\n",
    "    for iter in range(num_iters):\n",
    "        next_pr = {user.id : base_pr for user in users}  # start with base_pr\n",
    "        for source, target in endorsements:\n",
    "            # Add damped fraction of source pr to target\n",
    "            next_pr[target] += damping * pr[source] / outgoing_counts[source]\n",
    "        pr = next_pr\n",
    "\n",
    "    return pr\n",
    "\n",
    "pr = page_rank(users, endorsements)\n",
    "\n",
    "# Thor (user_id 4) has higher page rank than anyone else.\n",
    "# Even though Thor has fewer endorsements (two) than users 0, 1, and 2, his endorsements carry with them rank from their endorsements. \n",
    "# Additionally, both of his endorsers endorsed only him, which means that he doesn’t have to divide their rank with anyone else.\n",
    "assert pr[4] > max(page_rank\n",
    "                   for user_id, page_rank in pr.items()\n",
    "                   if user_id != 4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99a4e34db7e30f36b1ef5457f416347a7ea235dd25b8ab561424bdae8460da4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
